# ğŸ¤– ML RAG Assistant â€” Frontend Interface

The **ML RAG Assistant** is a sleek, browser-based UI that leverages **Retrieval-Augmented Generation (RAG)** to answer questions from machine learning PDFs. Users ask natural language queries, and the assistant retrieves and ranks relevant snippets with citations, offering an intuitive and informative Q&A experience.

> âš ï¸ **Note**: The backend is hosted on [Render](https://pdfquery-hm07.onrender.com). It may take **10â€“30 seconds** to respond after initial load if idle.

---

## ğŸ§  Research Foundation

This project is based by the paper:

**Kotsiantis, S. B. (2007). _Supervised Machine Learning: A Review of Classification Techniques_. Informatica, 31(3), 249â€“268**  
ğŸ”— [Read the Paper](https://www.researchgate.net/publication/220166738_Supervised_Machine_Learning_A_Review_of_Classification_Techniques)

The assistantâ€™s knowledge is based on this comprehensive review of supervised learning.

---

## ğŸš€ Key Features

- ğŸ“š **Smart PDF Q&A** â€” Ask ML questions and get answers with cited sources  
- ğŸ§  **OpenAI-Compatible LLM (GPTâ€‘4.1-nano)** â€” Lightweight, contextual, and fast  
- ğŸ¯ **Topâ€‘K Control** â€” Choose how many chunks to retrieve and rank  
- ğŸ§¾ **Cited Answers** â€” Source snippets, match scores, and page numbers  
- â™»ï¸ **Persistent Chat** â€” Stores history and preferences with localStorage  
- âœ… **Backend Health Monitor** â€” Live ping to check server availability  
- ğŸ“± **Responsive UI** â€” Optimized for desktop and mobile

---

## ğŸ§© Tech Stack

- **Frontend**: HTML, CSS, JavaScript (Vanilla)  
- **Icons**: FontAwesome  
- **Storage**: Browser LocalStorage  
- **Backend**: Flask API deployed on Render  
- **Retrieval**: FAISS + PDF chunking  
- **Model**: GPT-4.1-nano or compatible OpenAI API

---

## ğŸŒ Backend API

Base URL: `https://pdfquery-hm07.onrender.com`

| Endpoint     | Method | Purpose                    |
|--------------|--------|----------------------------|
| `/health`    | GET    | Check if backend is live   |
| `/ask`       | POST   | Submit question + top_k    |

âš ï¸ Cold starts can take 10â€“30 seconds on Render's free plan. Wait until backend status reads â€œBackend connected.â€

---

## ğŸ§ª Sample Use Cases

- â€œWhatâ€™s the difference between decision trees and random forests?â€  
- â€œIs logistic regression a generative model?â€  
- â€œHow does Naive Bayes handle missing data?â€  
- â€œCompare k-NN vs perceptron in supervised learning.â€  

Each response includes traceable citations to chunks from the Kotsiantis paper.

---

## ğŸ› ï¸ How It Works

1. User enters a question  
2. App sends query to backend `/ask` endpoint  
3. Backend retrieves Topâ€‘K matching chunks using vector similarity  
4. LLM generates a response using those sources  
5. Frontend displays the answer + sources, model tag, and tools (copy, expand)

---

## ğŸ“¦ Future Enhancements

- ğŸ” User-uploaded PDFs  
- ğŸ”„ Real-time collaborative sync  
- ğŸ§  Multi-model support (Claude, LLaMA, Gemini)  
- ğŸŒ i18n support  
- ğŸ–ï¸ Inline PDF highlighting of sources

---

## ğŸ‘¨â€ğŸ’» Author

**Mudit Mayank Jha**  
B.Sc. Computer Science @ UWI â€¢ Exchange Student @ University of Richmond  
ğŸ”— [GitHub](https://github.com/muditjha20)

---

## ğŸ“„ License

MIT â€” Free for personal and commercial use.

---

## ğŸ“š BibTeX Citation

```bibtex
@article{kotsiantis2007supervised,
  title={Supervised Machine Learning: A Review of Classification Techniques},
  author={Kotsiantis, S. B.},
  journal={Informatica},
  volume={31},
  number={3},
  pages={249--268},
  year={2007}
}
```
